{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team View - {{extract.name}}\n",
    "\n",
    "This notebook tells the story of the \"project flow\" for the {{extract.name}} project. This project is composed of the following repositories:\n",
    "\n",
    {% for repo in extract.repos %}"* {{repo.name}}\n",{% endfor %}
    "\n",
    "## Initialization\n",
    "\n",
    "To initialize this notebook, we will import standard libraries (pandas, numpy, holoviews, etc.), we'll define the data directory, and the resource types. We'll define any general utility functions for data manipulation and visualization. We will load the data into data frames. Finally, we will set any global visualization customizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# standard imports + load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "from itertools import product\n",
    "hv.extension('bokeh', logo=False)\n",
    "\n",
    "project_stats_dir = 'data/{{extract.name}}'\n",
    "\n",
    "# The following language types are considered \"resources\" rather than \"source code\"\n",
    "resource_types = ['Plain Text', 'Notebook', 'Markdown', 'CSS', 'HTML', 'JSON', \n",
    "                  'SVG', 'YAML', 'ReStructuredText', 'TOML']\n",
    "\n",
    "# use this when you don't want scientific notation on an axis\n",
    "from bokeh.models import BasicTickFormatter\n",
    "def apply_formatter_y_non_scientific(plot, element):\n",
    "    plot.handles['yaxis'].formatter = BasicTickFormatter(use_scientific=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "# There are commits that improperly bias the net contribution for an author (either up or down). \n",
    "# The following adjustments will be made to compensate for that bias. To utilize adjustment,  \n",
    "# look at the \"Commits to Consider for Exclusion\" section of this notebook. Based on the results\n",
    "# in that section, entries can be added here. Usually, you will want to copy the rows from loc_delta.csv,\n",
    "# reverse the sign on the count columns, and change the comment to explain why the adjustment\n",
    "# is being made.\n",
    "\n",
    "adjustments = StringIO('''\n",
    "Repo,CommitHash,TimeStamp,Author,Language,Files,Lines,Code,Comments,Blanks,Revision Comment\n",
    {% for adjustment in extract.adjustments %}"{{adjustment}}\n",{% endfor %}
    "''')\n",
    "\n",
    "# Time of day adjustments to bring every member of the team into the same \"work day\".\n",
    "tod_adjustments = [{% for adjustment in extract.tod_adjustments %}('{{adjustment[0]}}', {{adjustment[1]}}),{% endfor %}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def get_language_type(lang):\n",
    "    if lang in resource_types:\n",
    "        return 'Resource'\n",
    "    elif lang == 'Total':\n",
    "        return 'Total'\n",
    "    return 'Source'\n",
    "\n",
    "def augment_source(df):\n",
    "    '''Add source column based on language of row'''\n",
    "    df['Type'] = df.apply(lambda row: get_language_type(row['Language']), axis=1)\n",
    "    return df\n",
    "\n",
    "def augment_datetime(df):\n",
    "    '''Add useful datetime columns based on timestamp of row'''\n",
    "    df['Day'] = pd.to_datetime(df['TimeStamp'], unit='s').dt.date\n",
    "    df['Date'] = pd.to_datetime(df['TimeStamp'], unit='s').dt.tz_localize('UTC').dt.tz_convert('US/Mountain')\n",
    "    mindate = df['Date'].min()\n",
    "    df['Delta'] = df['Date'] - mindate\n",
    "    df['WeekOfProject'] = df['Delta'].dt.days // 7\n",
    "    df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "    df['HourOfDay'] = df['Date'].dt.hour\n",
    "    return df\n",
    "\n",
    "def adjust_for_timezone(df):\n",
    "    '''Adjust datetime columns based on timezone of author. Shift everyone into the \"same\" work schedule'''\n",
    "    for adjustment in tod_adjustments:\n",
    "        df['DayOfWeek'] = np.where(df['Author'] == adjustment[0], (df['Date'] + pd.DateOffset(hours=adjustment[1])).dt.weekday, df['DayOfWeek'])\n",
    "        df['HourOfDay'] = np.where(df['Author'] == adjustment[0], (df['Date'] + pd.DateOffset(hours=adjustment[1])).dt.hour, df['HourOfDay'])\n",
    "    return df\n",
    "\n",
    "def split_source_resource(df):\n",
    "    '''split a dataframe into its source and resource rows'''\n",
    "    df_src = df[df.Type == 'Source']\n",
    "    df_resrc = df[df.Type == 'Resource']\n",
    "    return df_src, df_resrc\n",
    "\n",
    "def split_pr(df):\n",
    "    '''split a dataframe into prs and other commits'''\n",
    "    df_pull_requests = df[df.MergeToMaster != 0]\n",
    "    df_commits = df[df.MergeToMaster == 0]\n",
    "    return df_pull_requests, df_commits\n",
    "\n",
    "def create_background_counts(df, x_range, y_range, columns):\n",
    "    '''Create a background heatmap with 0 counts across the board and then merge with results from \n",
    "    input dataframe'''\n",
    "    init = pd.DataFrame(list(product(df['Author'].drop_duplicates(), range(x_range), range(y_range), [0])), \n",
    "                        columns=columns)\n",
    "    df = init.merge(df, how='outer', on=columns[:-1])\n",
    "    df['Count'] = df.Count_x + df.Count_y\n",
    "    df.drop(['Count_x', 'Count_y'], axis=1)\n",
    "    return df\n",
    "\n",
    "def create_background_counts_dow(df):\n",
    "    '''Create a background heatmap with 0 counts across the board and then merge with results from \n",
    "    input dataframe'''\n",
    "    init = pd.DataFrame(list(product(range(df['WeekOfProject'].max() + 1), range(7), [0])), \n",
    "                        columns=['WeekOfProject', 'DayOfWeek', 'Count'])\n",
    "    df = init.merge(df, how='outer', on=['WeekOfProject', 'DayOfWeek'])\n",
    "    df['Count'] = df.Count_x + df.Count_y\n",
    "    df.drop(['Count_x', 'Count_y'], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_auth_tot = pd.read_csv(f'{project_stats_dir}/author_totals.csv')\n",
    "df_loc = augment_source(augment_datetime(pd.read_csv(f'{project_stats_dir}/loc.csv')))\n",
    "df_loc_delta = pd.read_csv(f'{project_stats_dir}/loc_delta.csv')\n",
    "df_revs = adjust_for_timezone(augment_datetime(pd.read_csv(f'{project_stats_dir}/revs.csv')))\n",
    "df_repo = augment_source(pd.read_csv(f'{project_stats_dir}/repo.csv'))\n",
    "df_prs = pd.read_csv(f'{project_stats_dir}/prs.csv')\n",
    "\n",
    "df_loc_delta = augment_source(pd.concat([\n",
    "    df_loc_delta,\n",
    "    pd.read_csv(adjustments)\n",
    "]))",
    "\n",
    "weeks_in_project = df_revs['WeekOfProject'].max() + 1\n",
    "dop_width = int(weeks_in_project * 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%opts Bars [tools=['hover'] xrotation=90 finalize_hooks=[apply_formatter_y_non_scientific]] {+framewise +axiswise}\n",
    "%opts Curve [tools=['hover'] xrotation=90] {+axiswise}\n",
    "%opts NdOverlay [finalize_hooks=[apply_formatter_y_non_scientific]]\n",
    "%opts HeatMap [tools=['hover'] xrotation=90]\n",
    "%opts HeatMap.DayOfWeek [height=150 width=350 xrotation=90]\n",
    "%opts HeatMap.HourOfDay [height=350 width=150]\n",
    "%opts HeatMap.DayOfProject [height=175 width=dop_width]\n",
    "%opts Histogram [tools=['hover']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volume of Source and Resource\n",
    "\n",
    "Let's look at just the volume of source files in the repositories for this project. Volume will be examined on a per repository / per language basis and is measured in terms of line count and file count. We will split the analysis into Source (C, Python, etc.) and Resource (Markdown, Plain Text, etc.) The reason for seperating Source and Resource is due to significant differences that we commonly see in range extents. Doing so provides a better understanding of the relative volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_lines_and_files(df, group):\n",
    "    ds = hv.Dataset(df, kdims=['Repo', 'Language'], vdims=['Lines', 'Files'])\n",
    "    return ds.to(hv.Bars, kdims='Language', vdims='Lines', group=group, label='Lines:') + ds.to(hv.Bars, kdims='Language', vdims='Files', group=group, label='Files:')\n",
    "\n",
    "df_src, df_resrc = split_source_resource(df_repo)\n",
    "\n",
    "src_holomap = get_lines_and_files(df_src, 'Source')\n",
    "resrc_holomap = get_lines_and_files(df_resrc, 'Resource')\n",
    "\n",
    "(src_holomap + resrc_holomap).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Contribution to Change by Author\n",
    "\n",
    "Let's look at the contribution for each author on a per source/resource, per language basis. This is the net contribution and as such it's possible that a given author may have a negative contribution for a given language. This is not necessarily a bad thing and can be an indicator or refactoring for simplicity/elegance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%output size=200\n",
    "%%opts Bars [legend_position='top' width=800 tools=['hover'] xrotation=90 finalize_hooks=[apply_formatter_y_non_scientific]] {+framewise +axiswise}\n",
    "\n",
    "df_src, df_resrc = split_source_resource(df_loc_delta)\n",
    "df_src = df_src.groupby(by=['Author', 'Language']).sum()\n",
    "df_resrc = df_resrc.groupby(by=['Author', 'Language']).sum()\n",
    "\n",
    "kdims = ['Author', 'Language']\n",
    "vdims = ['Code', 'Comments', 'Blanks']\n",
    "\n",
    "src_bars = hv.Bars(df_src, kdims, vdims, label='Source Code')\n",
    "resrc_bars = hv.Bars(df_resrc, kdims, vdims, label='Resources')\n",
    "\n",
    "(src_bars + resrc_bars).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commits by Author to Repository in Project\n",
    "\n",
    "To get an overview of the number of commits by an autor for each repository we'll generate a heatmap with axis of author and repository with the values being the number of commits for each author/repo combination. This helps show which repositories are key (they will have broad participation from the pool of authors), which authors function broadly (they will have participation across repositories) and which authors are prolific (they will have more \"warmer\" squares).\n",
    "\n",
    "Additionally, this shows repositories that may not have sufficient \"coverage\" if there is either a single author or a small number of authors contributing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%output size=200\n",
    "hv.HeatMap(df_auth_tot, kdims=['Repo', 'Author'], vdims='Commits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the same data in a pivot table for comparison of visual vs. numeric representation. Which do you prefer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "commits = pd.pivot_table(df_auth_tot, values=['Commits'], index=['Author'], columns=['Repo'], fill_value=0, aggfunc=np.sum)\n",
    "commits['Total'] = commits.sum(axis=1)\n",
    "commits.sort_values(by='Total', ascending=False).append(commits.sum().rename('Total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lines of Code Growth Over Time\n",
    "\n",
    "Let's explore the growth of LOC over time for each of the repositories. Note that repositories that have infrequent commits will have curves with visiblly longer line segments, while repositories with frequent commits will be \"smoother\". Also not that the Resource plot is log scale on the y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%opts NdOverlay [legend_position='right']\n",
    "%%opts NdOverlay.Resource [logy=True]\n",
    "%%opts Curve [width=700 height=400] {+axiswise}\n",
    "\n",
    "df_src, df_resrc = split_source_resource(df_loc.copy().drop(['CommitHash', 'TimeStamp', 'Language'], axis=1))\n",
    "\n",
    "df_src = df_src.groupby(by=['Repo', 'Day']).max()\n",
    "df_resrc = df_resrc.groupby(by=['Repo', 'Day']).max()\n",
    "\n",
    "kdims = ['Day', 'Repo']\n",
    "vdims = [('Code', 'Lines of Code')]\n",
    "\n",
    "src_curve = hv.Dataset(df_src, kdims, vdims).to(hv.Curve).overlay(group='Source')\n",
    "resrc_curve = hv.Dataset(df_resrc, kdims, vdims).to(hv.Curve).overlay(group='Resource')\n",
    "\n",
    "(src_curve + resrc_curve).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Contribution by Various Time Metrics\n",
    "\n",
    "Plot out author contribution as a function of time of day, day of week and week of project. What are some of the interesting patterns that you observe? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df = df_revs.copy()\n",
    "df['Count'] = 0\n",
    "\n",
    "tod_kdims = ['Author', 'DayOfWeek', 'HourOfDay']\n",
    "dop_kdims = ['Author', 'WeekOfProject', 'DayOfWeek']\n",
    "vdims = ['Count']\n",
    "\n",
    "df_tod = create_background_counts(df.groupby(tod_kdims).count().reset_index(), 7, 24, tod_kdims + vdims)\n",
    "df_dop = create_background_counts(df.groupby(dop_kdims).count().reset_index(), weeks_in_project, 7, dop_kdims + vdims)\n",
    "\n",
    "tod_heatmap = hv.Dataset(df_tod, tod_kdims, vdims).to(hv.HeatMap, kdims=['DayOfWeek', 'HourOfDay'], group=\"HourOfDay\", sort=True)\n",
    "dop_heatmap = hv.Dataset(df_dop, dop_kdims, vdims).to(hv.HeatMap, kdims=['WeekOfProject', 'DayOfWeek'], group=\"DayOfProject\", sort=True)\n",
    "\n",
    "(tod_heatmap + dop_heatmap).cols(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit Frequency\n",
    "\n",
    "Plot out general activity in the project as a heatmap with the wee of the project along the x axis and the day of the week along the y axis. Compare pull requests (which indicate updates to the staging/production environment) against general commits to the project. Frequent updates during weekends, especially of pull requests, can be a sign of \"emergency\" changes and could be indication of quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%opts HeatMap [height=150 width=300]\n",
    "\n",
    "df = df_revs.copy()\n",
    "df['Count'] = 0\n",
    "\n",
    "kdims = ['WeekOfProject', 'DayOfWeek']\n",
    "vdims = ['Count']\n",
    "\n",
    "df_pr, df_ct = split_pr(df)\n",
    "df_pr = create_background_counts_dow(df_pr.groupby(kdims).count().reset_index())\n",
    "df_ct = create_background_counts_dow(df_ct.groupby(kdims).count().reset_index())\n",
    "\n",
    "pr_heatmap = hv.Dataset(df_pr, kdims, vdims).to(hv.HeatMap, kdims, label='Pull Requests', group='DayOfWeek')\n",
    "ct_heatmap = hv.Dataset(df_ct, kdims, vdims).to(hv.HeatMap, kdims, label='Commits', group='DayOfWeek')\n",
    "\n",
    "(pr_heatmap + ct_heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration Between PR Creation and Merge\n",
    "\n",
    "Look at the overall distribution of the time between the last commit of a pull request and the time it was merged to\n",
    "the master branch. Lengthy durations may be an indication that there are snags in your development workflow processes or excessive volume of code in pull requests.\n",
    "\n",
    "Because most pull requests merge fairly quickly, we'll shave off the first two buckets to avoid having those overwelm the insights in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "frequencies, edges = np.histogram(df_prs['PrMergeDuration']/3600, 100)\n",
    "hist = hv.Histogram((edges[2:], frequencies[2:]), kdims=[hv.Dimension('Hours')])\n",
    "print(f\"{frequencies[0]} PRs merged in under {(edges[0] * 3600):2.3} seconds.\")\n",
    "print(f\"{frequencies[0] + frequencies[1]} PRs merged in under {edges[1]:2.3} hours.\")\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the PR that took the longest to merge. Record the repo and commit hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_prs[df_prs['PrMergeDuration'] == df_prs['PrMergeDuration'].max()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
